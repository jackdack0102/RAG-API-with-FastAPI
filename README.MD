RAG API Deployment & Automation

A production-style Retrieval-Augmented Generation (RAG) API built with FastAPI, integrating local document retrieval with a local Large Language Model (Ollama). This project demonstrates a fully local AI workflow with vector-based knowledge retrieval, clean API design, and developer-friendly documentation.

Features:

- FastAPI-based RAG API
- Clean and modular API structure with clear endpoints
- Built-in validation and interactive documentation via Swagger UI
- Local LLM Integration
- Uses Ollama to run a Large Language Model locally
- No external API costs required
- Vector-Based Knowledge Retrieval
- Implements Chroma vector database for semantic search
- Enables context-aware question answering from custom documents
- Production-Ready Design
- Modular service architecture
- Maintainable and scalable code structure
- Separation of concerns between retrieval, LLM, and API layers